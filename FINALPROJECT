import os
from pinecone import Pinecone
from transformers import AutoTokenizer, AutoModel
import torch
from groq import Groq

# === CONFIG ===
GROQ_API_KEY =   # <-- Replace
GROQ_MODEL = "compound-beta"   # <-- or llama3-70b-8192 etc
PINECONE_INDEX_NAME = "medical"
PINECONE_NAMESPACE = "ns2"

# === Load embedding model ===
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"âœ… Using device: {device}")

model_name = "Snowflake/snowflake-arctic-embed-m"
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
model = AutoModel.from_pretrained(model_name, trust_remote_code=True).to(device)

def get_embedding(text):
    inputs = tokenizer(
        text,
        return_tensors="pt",
        truncation=True,
        padding=True
    ).to(device)
    with torch.no_grad():
        outputs = model(**inputs)
        embeddings = outputs.last_hidden_state.mean(dim=1)
    return embeddings.squeeze().tolist()

# === Initialize Pinecone ===
pc = Pinecone(
index = pc.Index(PINECONE_INDEX_NAME)

# === Initialize Groq ===
groq_client = Groq(api_key=GROQ_API_KEY)

# === Search function ===
def retrieve_top_k(query, k=3):
    query_embedding = get_embedding(query)
    res = index.query(
        vector=query_embedding,
        top_k=k,
        namespace=PINECONE_NAMESPACE,
        include_metadata=True
    )
    matches = res["matches"]
    return [m["metadata"]["text"] for m in matches]

# === Prompt Template ===
CUSTOM_PROMPT_TEMPLATE = """
You are a helpful assistant. Use the comtect to provide answers if context is not relevant provide your own answer and provide a precise answer.

Context:
{context}

Question:
{question}

Answer:
"""

# === Main QA Function ===
def answer_question(user_query):
    retrieved_docs = retrieve_top_k(user_query, k=5)
    context = "\n\n".join(retrieved_docs)
    prompt = CUSTOM_PROMPT_TEMPLATE.format(context=context, question=user_query)
    
    response = groq_client.chat.completions.create(
        model=GROQ_MODEL,
        messages=[
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": prompt}
        ]
    )
    return response.choices[0].message.content, retrieved_docs

# === CLI Interface ===
if __name__ == "__main__":
    while True:
        query = input("\nEnter your question (or 'exit'): ").strip()
        if query.lower() in {"exit", "quit"}:
            break
        answer, sources = answer_question(query)
        print("\nâœ… ANSWER:\n", answer)
        print("\nðŸ“„ Retrieved Chunks:\n")
        for i, chunk in enumerate(sources, 1):
            print(f"{i}. {chunk[:200]}...")